import os
import sys
import gc
import time
import math
import glob
import shutil
import datetime
import threading
import subprocess
import requests
import json
import re  # <--- –î–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–æ–º–µ—Ä–∞ —à–∞–≥–∞
import pandas as pd
import numpy as np
import torch
import gradio as gr
import sentencepiece as spm

# üî• OPTIMIZATIONS: High Precision for Ampere+ GPUs
torch.set_float32_matmul_precision('high')

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
os.environ["TOKENIZERS_PARALLELISM"] = "true"
os.environ["OMP_NUM_THREADS"] = "8"

from transformers import LlamaConfig, LlamaForCausalLM, LlamaTokenizerFast
from safetensors.torch import save_file, load_file # <--- load_file –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏

# --- CHECKS ---
try:
    import bulba_rust
    HAS_RUST = True
except ImportError:
    print("‚ùå CRITICAL: 'bulba_rust' not found! Run: maturin develop --release")
    HAS_RUST = False

try:
    import bitsandbytes as bnb
    HAS_BNB = True
except ImportError: HAS_BNB = False

# --- DIRS ---
DIRS = {
    "DATA": "data",
    "CHECKPOINTS": "checkpoints",
    "MODELS": "output_models",
    "SYSTEM": "system",
    "TOKENIZER": os.path.join("system", "tokenizer"),
    "TOOLS": os.path.join("system", "tools"),
    "TEMP": os.path.join("system", "temp_convert"),
    "TEMP_TRAIN": os.path.join("system", "temp_train_tok")
}
for d in DIRS.values(): os.makedirs(d, exist_ok=True)
TRAIN_BIN_PATH = os.path.join(DIRS["DATA"], "train.bin")

# üî• PRESETS
PRESETS = {
    "Krolik (50M)":  { "h": 512,  "i": 1376, "l": 8,  "hd": 8,  "kv": 4, "ctx": 512, "bs": 8, "acc": 8, "steps": 5000 },
    "AIst (150M)":   { "h": 768,  "i": 2048, "l": 12, "hd": 12, "kv": 4, "ctx": 512, "bs": 8, "acc": 8, "steps": 10000 },
    "Zubr (350M)":   { "h": 1024, "i": 2816, "l": 28, "hd": 16, "kv": 4, "ctx": 512, "bs": 2, "acc": 32, "steps": 15000 },
}

def format_size(size_bytes):
    if size_bytes == 0: return "0 B"
    i = int(math.floor(math.log(size_bytes, 1024)))
    return "%s %s" % (round(size_bytes / math.pow(1024, i), 2), ("B", "KB", "MB", "GB")[i])

def get_lr(step, max_steps, max_lr):
    warmup = int(max_steps * 0.05)
    if step < warmup: return max_lr * (step + 1) / warmup
    if step > max_steps: return max_lr * 0.1
    decay = (step - warmup) / (max_steps - warmup)
    return max_lr * 0.1 + 0.5 * (1.0 + math.cos(math.pi * decay)) * (max_lr - max_lr * 0.1)

def check_system_status():
    bin_exists = os.path.exists(TRAIN_BIN_PATH)
    tok_path = os.path.join(DIRS["TOKENIZER"], "tokenizer.model")
    status = []
    if bin_exists: status.append(f"‚úÖ Data: {format_size(os.path.getsize(TRAIN_BIN_PATH))}")
    else: status.append("‚ùå Data Missing")
    if os.path.exists(tok_path): status.append("‚úÖ SentencePiece Ready")
    else: status.append("‚ùå Tokenizer Missing")
    return " | ".join(status)

def scan_checkpoints():
    files = sorted(glob.glob(f"{DIRS['CHECKPOINTS']}/*.safetensors"), key=os.path.getmtime, reverse=True)
    if not files: return gr.Dropdown(choices=[], value=None, label="No checkpoints found")
    return gr.Dropdown(choices=files, value=None, label="Resume from Checkpoint (Optional)")

# ================= üÜï SENTENCEPIECE TRAINING =================

def train_sentencepiece_model(files, vocab_size, output_dir):
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(DIRS["TEMP_TRAIN"], exist_ok=True)
    
    print("üìù Preparing data for tokenizer training...")
    big_text_file = os.path.join(DIRS["TEMP_TRAIN"], "corpus.txt")
    total_written = 0
    limit = 100 * 1024 * 1024 
    
    with open(big_text_file, "w", encoding="utf-8") as out:
        for fpath in files:
            try:
                with open(fpath, "r", encoding="utf-8", errors="ignore") as f:
                    for line in f:
                        if line.strip():
                            out.write(line)
                            total_written += len(line.encode("utf-8"))
                            if total_written > limit: break
            except: pass
            if total_written > limit: break
            
    print("üß† Training SentencePiece BPE...")
    model_prefix = os.path.join(output_dir, "tokenizer")
    
    spm.SentencePieceTrainer.train(
        input=big_text_file,
        model_prefix=model_prefix,
        vocab_size=vocab_size,
        model_type="bpe",
        user_defined_symbols=["<|pad|>", "<|bos|>", "<|eos|>"],
        byte_fallback=True,
        train_extremely_large_corpus=False,
        character_coverage=1.0,
        unk_id=0, bos_id=1, eos_id=2, pad_id=-1
    )
    
    print("üîÑ Converting to HuggingFace Fast format...")
    tokenizer = LlamaTokenizerFast(vocab_file=model_prefix + ".model")
    tokenizer.bos_token = "<s>"
    tokenizer.eos_token = "</s>"
    tokenizer.unk_token = "<unk>"
    tokenizer.pad_token = "<|pad|>"
    
    tokenizer.save_pretrained(output_dir)
    shutil.rmtree(DIRS["TEMP_TRAIN"], ignore_errors=True)
    print(f"‚úÖ Tokenizer saved to {output_dir}")

# ================= üöÄ PROCESSING =================

def process_data(upload_files, local_path, progress=gr.Progress()):
    """
    1. –¢—Ä–µ–Ω–∏—Ä—É–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (–µ—Å–ª–∏ –Ω–µ—Ç).
    2. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –≤ —Ç–æ–∫–µ–Ω—ã (—á–µ—Ä–µ–∑ Rust).
    3. –ü–ï–†–ï–ú–ï–®–ò–í–ê–ï–¢ –¥–∞–Ω–Ω—ã–µ (—á–µ—Ä–µ–∑ Rust).
    """
    if not HAS_RUST: 
        return "‚ùå Rust Missing", None, 0, pd.DataFrame(), "Error"
    
    gc.collect()
    
    # --- 1. –°–±–æ—Ä —Ñ–∞–π–ª–æ–≤ ---
    files = []
    search_path = local_path if local_path else DIRS["DATA"]
    if upload_files: 
        files = [f.name for f in upload_files]
    elif os.path.exists(search_path):
        if os.path.isfile(search_path): 
            files = [search_path]
        else:
            for ext in ['.txt', '.json', '.md', '.csv', '.py']:
                files.extend(glob.glob(os.path.join(search_path, "**", ext), recursive=True))

    if not files: 
        return f"‚ö†Ô∏è No files in {search_path}", TRAIN_BIN_PATH, 0, pd.DataFrame(), "Error"

    # --- 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ ---
    tok_dir = DIRS["TOKENIZER"]
    if not os.path.exists(os.path.join(tok_dir, "tokenizer.model")):
        progress(0.1, desc="üáßüáæ Training SentencePiece...")
        try:
            train_sentencepiece_model(files, 32000, tok_dir)
        except Exception as e:
            return f"‚ùå SPM Train Error: {e}", None, 0, pd.DataFrame(), "Error"

    # --- 3. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è (Rust) ---
    progress(0.3, desc="üáßüáæ Rust: Tokenizing...")
    
    # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–π —Ñ–∞–π–ª –æ–±—É—á–µ–Ω–∏—è –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º
    if os.path.exists(TRAIN_BIN_PATH): 
        os.remove(TRAIN_BIN_PATH)

    abs_files = [os.path.abspath(f) for f in files]
    abs_tok = os.path.abspath(os.path.join(tok_dir, "tokenizer.json"))
    abs_out = os.path.abspath(TRAIN_BIN_PATH)

    task = {"done": False, "tokens": 0}
    
    def run_rust():
        try:
            # –ó–∞–ø—É—Å–∫–∞–µ–º Rust —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            res = bulba_rust.process_parallel(abs_files, abs_tok, abs_out)
            task["tokens"] = int(res)
        except Exception as e: 
            print(f"Rust Error: {e}")
        finally: 
            task["done"] = True

    t = threading.Thread(target=run_rust)
    t.start()
    
    hist_data = []
    start_time = time.time()
    
    # –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ —Å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º –≥—Ä–∞—Ñ–∏–∫–∞
    while t.is_alive():
        time.sleep(0.5)
        elapsed = time.time() - start_time
        if elapsed < 0.1: continue
        
        if os.path.exists(TRAIN_BIN_PATH):
            cur_sz = os.path.getsize(TRAIN_BIN_PATH)
            avg_speed = (cur_sz / (1024*1024)) / elapsed
            hist_data.append({"time": elapsed, "speed": avg_speed})
            
            if len(hist_data) > 100: display_data = hist_data[-100:]
            else: display_data = hist_data
                
            df = pd.DataFrame(display_data)
            yield f"Tokenizing... {format_size(cur_sz)} | Avg: {avg_speed:.1f} MB/s", TRAIN_BIN_PATH, 0, df, "Working..."
        
    t.join() # –ü–æ—Ç–æ–∫ –∑–∞–≤–µ—Ä—à–µ–Ω

    # --- 4. –ü–ï–†–ï–ú–ï–®–ò–í–ê–ù–ò–ï (SHUFFLE) ---
    # –≠—Ç–æ –∫–ª—é—á–µ–≤–æ–π –º–æ–º–µ–Ω—Ç –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å–∫–∞—á–∫–æ–≤ Loss!
    if task["done"] and os.path.exists(TRAIN_BIN_PATH):
        try:
            progress(0.9, desc="üé≤ Shuffling dataset (Rust)...")
            yield "üé≤ Shuffling dataset... Please wait...", TRAIN_BIN_PATH, 0, pd.DataFrame(hist_data), "Shuffling..."
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º EOS ID (–û–±—ã—á–Ω–æ 2 –¥–ª—è SentencePiece)
            eos_id = 2
            try:
                # –ü—ã—Ç–∞–µ–º—Å—è —Å—á–∏—Ç–∞—Ç—å —Ä–µ–∞–ª—å–Ω—ã–π EOS ID –∏–∑ tokenizer.json
                with open(abs_tok, 'r', encoding='utf-8') as f:
                    tj = json.load(f)
                    # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –º–æ–∂–µ—Ç –æ—Ç–ª–∏—á–∞—Ç—å—Å—è, –Ω–æ —á–∞—Å—Ç–æ —ç—Ç–æ model -> eos_token_id
                    # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–º - –æ—Å—Ç–∞–≤–∏–º 2
                    pass 
            except: pass
            
            # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º train.bin -> temp_unshuffled.bin
            temp_bin = os.path.join(DIRS["DATA"], "temp_unshuffled.bin")
            if os.path.exists(temp_bin): os.remove(temp_bin)
            os.rename(TRAIN_BIN_PATH, temp_bin)
            
            print(f"üé≤ Running Rust Shuffle on {format_size(os.path.getsize(temp_bin))}...")
            
            # –í—ã–∑—ã–≤–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –∏–∑ shuffler.rs
            bulba_rust.shuffle_dataset(temp_bin, TRAIN_BIN_PATH, eos_id)
            
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            if os.path.exists(temp_bin): os.remove(temp_bin)
            print("‚úÖ Shuffle done!")
            
        except Exception as e:
            print(f"‚ùå Shuffle Warning: {e}")
            # –ï—Å–ª–∏ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏–µ —É–ø–∞–ª–æ, –≤–µ—Ä–Ω–µ–º —Ñ–∞–π–ª –Ω–∞–∑–∞–¥
            if os.path.exists(temp_bin) and not os.path.exists(TRAIN_BIN_PATH):
                os.rename(temp_bin, TRAIN_BIN_PATH)
    
    final_sz = os.path.getsize(TRAIN_BIN_PATH) if os.path.exists(TRAIN_BIN_PATH) else 0
    yield f"‚úÖ DONE! Tokens: {task['tokens']:,} | Size: {format_size(final_sz)}", TRAIN_BIN_PATH, 0, pd.DataFrame(hist_data), "Done"

# ================= üß† TRAINING LOOP =================

STOP_FLAG = False
SAVE_FLAG = False 

def save_safe(model, path):
    folder = os.path.dirname(path)
    if not os.path.exists(folder): os.makedirs(folder, exist_ok=True)
    raw = model._orig_mod if hasattr(model, "_orig_mod") else model
    sd = raw.state_dict()
    if "lm_head.weight" in sd and "model.embed_tokens.weight" in sd:
        if sd["lm_head.weight"].data_ptr() == sd["model.embed_tokens.weight"].data_ptr():
            del sd["lm_head.weight"]
    save_file(sd, path)
    print(f"üíæ Saved: {path}")

def train_loop(train_path, model_name, lr, steps_ui, mode, resume_path, progress=gr.Progress()):
    global STOP_FLAG, SAVE_FLAG
    STOP_FLAG = False
    SAVE_FLAG = False
    
    gc.collect()
    torch.cuda.empty_cache()
    
    target_bin = train_path if train_path else TRAIN_BIN_PATH
    if not os.path.exists(target_bin): return f"‚ùå Data not found", None, pd.DataFrame()
    
    tok_path = DIRS["TOKENIZER"]
    try: 
        tok = LlamaTokenizerFast.from_pretrained(tok_path)
    except: return "‚ùå Tokenizer Error", None, pd.DataFrame()

    real_vocab_size = len(tok)
    print(f"üìä Vocab: {real_vocab_size}")

    p = PRESETS[model_name]
    max_steps = int(steps_ui)
    device = torch.device('cuda')
    
    conf = LlamaConfig(
        vocab_size=real_vocab_size, hidden_size=p["h"], intermediate_size=p["i"],
        num_hidden_layers=p["l"], num_attention_heads=p["hd"], num_key_value_heads=p["kv"],
        max_position_embeddings=p["ctx"], architectures=["LlamaForCausalLM"],
        attn_implementation="sdpa"
    )
    
    model = LlamaForCausalLM(conf).to(device)
    
    # === RESUME LOGIC ===
    start_step = 0
    if resume_path and os.path.exists(resume_path):
        print(f"üîÑ Resuming from: {resume_path}")
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞
            state_dict = load_file(resume_path)
            model.load_state_dict(state_dict, strict=False)
            
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –Ω–æ–º–µ—Ä —à–∞–≥–∞ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä step_500.safetensors)
            match = re.search(r"(\d+)", os.path.basename(resume_path))
            if match:
                start_step = int(match.group(1))
                print(f"‚è© Starting from step {start_step}")
            else:
                print("‚ö†Ô∏è Step number not found in filename, starting from 0 but with loaded weights.")
        except Exception as e:
            print(f"‚ùå Failed to load checkpoint: {e}")
            return f"‚ùå Resume Error: {e}", None, pd.DataFrame()
    else:
        print("üÜï Starting training from scratch")
    # ====================
    
    print("üöÄ SPEED MODE: Gradient Checkpointing DISABLED (Fastest)")

    if "Compile" in mode or "Both" in mode:
        print("‚ö° Compiling Model (reduce-overhead)...")
        try:
            model = torch.compile(model, mode="reduce-overhead")
        except Exception as e:
            print(f"‚ö†Ô∏è Compile failed, continuing standard: {e}")

    model.train()

    if HAS_BNB:
        print("‚öñÔ∏è Using 8-bit AdamW (Balance Speed/Mem)")
        optim = bnb.optim.AdamW8bit(model.parameters(), lr=lr)
    else:
        print("‚ö†Ô∏è BNB Missing. Using Fused AdamW")
        optim = torch.optim.AdamW(model.parameters(), lr=lr, fused=True)
    
    scaler = torch.amp.GradScaler()
    
    try: loader = bulba_rust.RustDataLoader(target_bin, tok.eos_token_id, True)
    except Exception as e: return f"‚ùå Loader Error: {e}", None, pd.DataFrame()

    hist_tr = []
    avg_loss = 0
    t0 = time.time()
    
    expected_len = p["bs"] * (p["ctx"] + 1)

    print(f"üöÄ STARTING LOOP ({start_step} -> {max_steps})...")

    # üî• Loop starts from start_step
    for step in range(start_step, max_steps):
        if STOP_FLAG: break
        
        current_lr = get_lr(step, max_steps, lr)
        for g in optim.param_groups: g['lr'] = current_lr
        
        optim.zero_grad(set_to_none=True) 
        
        loss_accum = 0
        for _ in range(p["acc"]):
            raw = loader.next_batch(p["bs"], p["ctx"] + 1)
            
            data_np = np.frombuffer(raw, dtype=np.uint16).astype(np.int64)

            if data_np.size > expected_len:
                data_np = data_np[:expected_len]

            data = torch.from_numpy(data_np).view(p["bs"], p["ctx"] + 1).to(device, non_blocking=True)
            data.clamp_(max=real_vocab_size-1)

            with torch.amp.autocast("cuda"):
                loss = model(data[:, :-1], labels=data[:, 1:]).loss / p["acc"]
            
            scaler.scale(loss).backward()
            loss_accum += loss.item()
        
        scaler.unscale_(optim)
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        
        scaler.step(optim)
        scaler.update()
        
        if step == start_step: avg_loss = loss_accum
        else: avg_loss = 0.95 * avg_loss + 0.05 * loss_accum
        
        if SAVE_FLAG:
            save_safe(model, f"{DIRS['CHECKPOINTS']}/manual_{step}.safetensors")
            SAVE_FLAG = False

        if step % 10 == 0:
            dt = time.time() - t0
            if dt == 0: dt = 0.001
            # –ö–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ä–∞—Å—á–µ—Ç —Å–∫–æ—Ä–æ—Å—Ç–∏ —Å —É—á–µ—Ç–æ–º —Å—Ç–∞—Ä—Ç–æ–≤–æ–≥–æ —à–∞–≥–∞
            steps_done = step - start_step + 1
            spd = int(steps_done * p["bs"] * p["acc"] * p["ctx"] / dt)
            eta = str(datetime.timedelta(seconds=int((max_steps - step) / (steps_done/dt))))
            hist_tr.append({"step": step, "loss": avg_loss})
            
            if step % 500 == 0 and step > 0: 
                save_safe(model, f"{DIRS['CHECKPOINTS']}/step_{step}.safetensors")
                
            yield f"Step: {step} | Loss: {avg_loss:.4f} | Speed: {spd} tok/s | ETA: {eta}", pd.DataFrame(hist_tr)

    save_safe(model, f"{DIRS['CHECKPOINTS']}/final.safetensors")
    yield "‚úÖ Done!", pd.DataFrame(hist_tr)

def stop_train(): global STOP_FLAG; STOP_FLAG = True; return "üõë Stopping..."
def trigger_save(): global SAVE_FLAG; SAVE_FLAG = True; return "üíæ Saving..." 

# ================= üì¶ EXPORT (MANUAL & ROBUST) =================

def auto_export_gguf(selected_ckpt, model_name_ui):
    if not selected_ckpt: return "‚ùå No checkpoint selected"
    yield f"Selected: {selected_ckpt}"
    
    script_path = os.path.join(DIRS["TOOLS"], "convert_hf_to_gguf.py")
    if not os.path.exists(script_path):
        yield "üì• Downloading converter script..."
        try:
            url = "https://raw.githubusercontent.com/ggerganov/llama.cpp/master/convert_hf_to_gguf.py"
            response = requests.get(url)
            if response.status_code == 200:
                with open(script_path, "wb") as f: f.write(response.content)
            else: return "‚ùå Failed to download convert script"
        except Exception as e: return f"‚ùå Network error: {e}"

    hf_path = DIRS["TEMP"]
    if os.path.exists(hf_path): shutil.rmtree(hf_path)
    os.makedirs(hf_path, exist_ok=True)
    
    try:
        yield "üìÇ Preparing files..."
        
        tok_model_src = os.path.join(DIRS["TOKENIZER"], "tokenizer.model")
        if not os.path.exists(tok_model_src):
            return "‚ùå Tokenizer.model not found! Train tokenizer first."
        shutil.copy(tok_model_src, os.path.join(hf_path, "tokenizer.model"))

        p = PRESETS.get(model_name_ui, PRESETS["AIst (150M)"])
        
        manual_config = {
            "architectures": ["LlamaForCausalLM"],
            "model_type": "llama",
            "vocab_size": 32000,
            "hidden_size": p["h"],
            "intermediate_size": p["i"],
            "num_hidden_layers": p["l"],
            "num_attention_heads": p["hd"],
            "num_key_value_heads": p["kv"],
            "max_position_embeddings": p["ctx"],
            "rope_theta": 10000.0,
            "bos_token_id": 1,
            "eos_token_id": 2,
            "rms_norm_eps": 1e-5
        }
        
        with open(os.path.join(hf_path, "config.json"), "w") as f:
            json.dump(manual_config, f, indent=2)

        shutil.copy(selected_ckpt, os.path.join(hf_path, "model.safetensors"))
        
        yield "üöÄ Running GGUF conversion..."
        ckpt_name = os.path.basename(selected_ckpt).replace(".safetensors", "")
        out_file = os.path.join(DIRS["MODELS"], f"bulba_{ckpt_name}.gguf")
        
        cmd = [sys.executable, script_path, hf_path, "--outfile", out_file, "--outtype", "f16"]
        
        result = subprocess.run(
            cmd, 
            capture_output=True, 
            text=True, 
            encoding='utf-8', 
            errors='replace'
        )
        
        if result.returncode == 0:
            yield f"‚úÖ SUCCESS!\nSaved to: {out_file}\n\n=== LOGS ===\n{result.stderr}"
        else:
            yield f"‚ùå FAILED!\n\n=== STDERR ===\n{result.stderr}\n\n=== STDOUT ===\n{result.stdout}"

    except Exception as e: yield f"‚ùå Python Error: {e}"

# ================= UI =================

def update_steps(name): return gr.Number(value=PRESETS[name]["steps"])

with gr.Blocks(title="BulbaGPT Studio") as demo:
    gr.Markdown("# ü•î BulbaGPT Studio: Resume Edition")
    s_path = gr.State(TRAIN_BIN_PATH)
    s_data_dir = gr.State(DIRS["DATA"])

    with gr.Tabs():
        with gr.Tab("üöÄ Train"):
            with gr.Row():
                with gr.Column():
                    files = gr.File(label="Files", file_count="multiple")
                    btn_proc = gr.Button("1. Prepare Data & Tokenizer", variant="secondary")
                    status_proc = gr.Textbox(label="Status")
                    plot_proc = gr.LinePlot(x="time", y="speed", title="Processing Speed (MB/s)", height=250)
                    
                    model_sel = gr.Radio(list(PRESETS.keys()), value="AIst (150M)", label="Config")
                    steps = gr.Number(label="Steps", value=10000)
                    model_sel.change(update_steps, model_sel, steps)
                    mode = gr.Dropdown(["Compile", "Flash Attention", "Both"], value="Both", label="Mode")
                    
                    # üî• NEW: Resume Option
                    with gr.Row():
                        resume_ckpt = gr.Dropdown(label="Resume from Checkpoint (Optional)", choices=[], value=None, scale=3)
                        btn_refresh_res = gr.Button("üîÑ", scale=1)

                    with gr.Row():
                        btn_run = gr.Button("2. START", variant="primary")
                        btn_save = gr.Button("Save")
                        btn_stop = gr.Button("Stop")
                with gr.Column():
                    status_train = gr.Textbox(label="Log")
                    plot = gr.LinePlot(x="step", y="loss", title="Loss", height=300)
        
        with gr.Tab("üì¶ Export"):
            ckpt = gr.Dropdown(label="Checkpoint")
            btn_ref = gr.Button("Refresh")
            btn_exp = gr.Button("Convert GGUF", variant="primary")
            log_exp = gr.Textbox()

    demo.load(check_system_status, outputs=status_proc)
    demo.load(scan_checkpoints, outputs=ckpt)
    demo.load(scan_checkpoints, outputs=resume_ckpt) # Scan checkpoints on load for resume
    
    btn_proc.click(process_data, [files, s_data_dir], [status_proc, s_path, gr.State(), plot_proc, gr.State()])
    
    # üî• Pass resume_ckpt to train_loop
    btn_run.click(train_loop, [s_path, model_sel, gr.Number(3e-4, visible=False), steps, mode, resume_ckpt], [status_train, plot])
    
    btn_stop.click(stop_train, outputs=status_train)
    btn_save.click(trigger_save, outputs=status_train) 
    
    btn_ref.click(scan_checkpoints, outputs=ckpt)
    btn_refresh_res.click(scan_checkpoints, outputs=resume_ckpt) # Button to refresh resume list
    
    btn_exp.click(auto_export_gguf, inputs=[ckpt, model_sel], outputs=log_exp)

if __name__ == "__main__":
    demo.queue().launch()