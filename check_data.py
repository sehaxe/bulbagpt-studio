import os
import struct
from transformers import LlamaTokenizerFast

# –ü—É—Ç–∏
BIN_PATH = "data/train.bin"
TOK_PATH = "system/tokenizer"

def inspect():
    if not os.path.exists(BIN_PATH):
        print("‚ùå –ù–µ—Ç —Ñ–∞–π–ª–∞ train.bin")
        return

    print(f"üîç –ß–∏—Ç–∞–µ–º {BIN_PATH}...")
    
    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    try:
        tokenizer = LlamaTokenizerFast.from_pretrained(TOK_PATH)
        print(f"‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω. EOS ID: {tokenizer.eos_token_id}")
    except:
        print("‚ùå –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return

    # 2. –ß–∏—Ç–∞–µ–º –ø–µ—Ä–≤—ã–µ 200 —Ç–æ–∫–µ–Ω–æ–≤ (—á–∏—Å–µ–ª) –∏–∑ –±–∏–Ω–∞—Ä–Ω–∏–∫–∞
    with open(BIN_PATH, "rb") as f:
        # –ß–∏—Ç–∞–µ–º 400 –±–∞–π—Ç (—Ç–∞–∫ –∫–∞–∫ uint16 = 2 –±–∞–π—Ç–∞)
        raw_data = f.read(400)
        # –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –≤ —á–∏—Å–ª–∞
        tokens = struct.unpack(f"{len(raw_data)//2}H", raw_data)

    print(f"\nüî¢ –ü–µ—Ä–≤—ã–µ 20 —Ç–æ–∫–µ–Ω–æ–≤ (ID): {tokens[:20]}")
    
    # 3. –î–µ–∫–æ–¥–∏—Ä—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ç–µ–∫—Å—Ç
    decoded_text = tokenizer.decode(tokens)
    
    print("\nüìú –í–û–¢ –ß–¢–û –í–ò–î–ò–¢ –ù–ï–ô–†–û–°–ï–¢–¨ (–ü–µ—Ä–≤—ã–µ 200 —Ç–æ–∫–µ–Ω–æ–≤):")
    print("="*40)
    print(decoded_text)
    print("="*40)

if __name__ == "__main__":
    inspect()