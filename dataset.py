import os
import re
import glob
import random
import pandas as pd
from datasets import load_dataset
from tqdm import tqdm
import gc

# ================= CONFIGURATION =================

DATA_DIR = "data"
OUTPUT_FILE = os.path.join(DATA_DIR, "aist_150m_mixed.txt")

LOCAL_PURE_TEXT_FILE = os.path.join(DATA_DIR, "pure_bel_books.txt") 
LOCAL_CULTURAX_PATH = "/home/sehaxe/bulbagpt/data/CulturalX_bel"

# ‚öôÔ∏è –õ–Ü–ú–Ü–¢–´
UPSAMPLE_INSTRUCT = 5     
UPSAMPLE_PURE_TEXT = 3    
LIMIT_CULTURAX = 150_000  
LIMIT_PYTHON = 30_000     
LIMIT_LOGIC_EN = 70_000   

MIN_TEXT_LENGTH = 300     
BUFFER_SIZE = 20000       # üî• –°–±—Ä–æ—Å –Ω–∞ –¥–∏—Å–∫ –∫–∞–∂–¥—ã–µ 20k –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–±–µ—Ä–µ–∂–µ—Ç RAM)

# ================= –§–£–ù–ö–¶–´–Ü –ê–ß–´–°–¢–ö–Ü =================

def sanitize_text(text):
    if not text: return ""
    text = re.sub(r"<\|im_start\|>.*?\n", "", text)
    text = text.replace("<|im_end|>", "").replace("<|endoftext|>", "")
    text = text.replace("<|begin_of_text|>", "")
    text = text.replace("<|start_header_id|>", "")
    text = text.replace("<|end_header_id|>", "")
    text = text.replace("<|eot_id|>", "")
    text = text.replace("<|end_of_text|>", "")
    return text.strip()

def is_pure_belarusian(text):
    if not text: return False
    text_lower = text.lower()
    if not re.search(r'[—û—ñ]', text_lower): return False
    total_chars = len(text)
    if total_chars < 50: return False 
    bad_chars = len(re.findall(r'[—â—ä–∏]', text_lower))
    if (bad_chars / total_chars) > 0.01: return False
    return True

def format_llama3_instruct(system, user, assistant):
    sys = sanitize_text(system) or "–¢—ã —Ä–∞–∑—É–º–Ω—ã —ñ –∫–∞—Ä—ã—Å–Ω—ã –ø–∞–º–æ—á–Ω—ñ–∫."
    usr = sanitize_text(user)
    ast = sanitize_text(assistant)
    if not usr or not ast: return ""
    return (
        f"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n{sys}<|eot_id|>"
        f"<|start_header_id|>user<|end_header_id|>\n\n{usr}<|eot_id|>"
        f"<|start_header_id|>assistant<|end_header_id|>\n\n{ast}<|eot_id|><|end_of_text|>\n"
    )

def format_pretrain(text):
    text = sanitize_text(text)
    if len(text) < MIN_TEXT_LENGTH: return ""
    return f"<|begin_of_text|>{text}<|end_of_text|>\n"

# ================= GENERATORS (STREAMING) =================
# –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ã –≤—ã–¥–∞—é—Ç –ø–æ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ—á–∫–µ –∑–∞ —Ä–∞–∑, –Ω–µ –∑–∞–Ω–∏–º–∞—è –ø–∞–º—è—Ç—å

def stream_books():
    """–ß–∏—Ç–∞–µ—Ç —Ñ–∞–π–ª –∫–Ω–∏–≥ —á–∞–Ω–∫–∞–º–∏, –Ω–µ –∑–∞–≥—Ä—É–∂–∞—è —Ü–µ–ª–∏–∫–æ–º"""
    if not os.path.exists(LOCAL_PURE_TEXT_FILE): return
    print("üìò Init Books stream...")
    try:
        # –ß–∏—Ç–∞–µ–º –ø–æ—Å—Ç—Ä–æ—á–Ω–æ –∏–ª–∏ –±–ª–æ–∫–∞–º–∏, –Ω–∞–∫–∞–ø–ª–∏–≤–∞—è –∞–±–∑–∞—Ü—ã
        current_chunk = ""
        with open(LOCAL_PURE_TEXT_FILE, "r", encoding="utf-8") as f:
            for line in f:
                if line.strip() == "": # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ - —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –∞–±–∑–∞—Ü–µ–≤
                    if len(current_chunk) > MIN_TEXT_LENGTH:
                        formatted = format_pretrain(current_chunk)
                        for _ in range(UPSAMPLE_PURE_TEXT):
                            yield formatted
                        current_chunk = ""
                    else:
                        current_chunk += "\n" # –ü—Ä–æ—Å—Ç–æ –ø–µ—Ä–µ–Ω–æ—Å, –µ—Å–ª–∏ –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞
                else:
                    current_chunk += line
        # –ü–æ—Å–ª–µ–¥–Ω–∏–π –∫—É—Å–æ–∫
        if len(current_chunk) > MIN_TEXT_LENGTH:
             formatted = format_pretrain(current_chunk)
             for _ in range(UPSAMPLE_PURE_TEXT): yield formatted
    except Exception as e: print(f"‚ùå Books Error: {e}")

def stream_alpaca():
    print("üí¨ Init Alpaca stream...")
    try:
        ds = load_dataset("saillab/alpaca-belarusian-cleaned", split="train", streaming=True)
        # –ü–æ—Å–∫–æ–ª—å–∫—É –¥–∞—Ç–∞—Å–µ—Ç –º–∞–ª–µ–Ω—å–∫–∏–π, –º–æ–∂–Ω–æ –ø—Ä–æ—á–∏—Ç–∞—Ç—å –µ–≥–æ, –Ω–æ yield-–∏—Ç—å –ø–æ –æ–¥–Ω–æ–º—É
        # Streaming mode –¥–ª—è HuggingFace datasets –Ω–µ –≥—Ä—É–∑–∏—Ç RAM
        for row in ds:
            user_msg = f"{row.get('instruction','')} {row.get('input','')}"
            text = format_llama3_instruct("", user_msg, row.get('output',''))
            if text:
                for _ in range(UPSAMPLE_INSTRUCT): # Upsample "on the fly"
                    yield text
    except Exception as e: print(f"‚ùå Alpaca Error: {e}")

def stream_python():
    print("üêç Init Python stream...")
    try:
        ds = load_dataset("iamtarun/python_code_instructions_18k_alpaca", split="train", streaming=True)
        count = 0
        for row in ds:
            if count >= LIMIT_PYTHON: break
            prompt = row.get('instruction', '') + "\n" + row.get('input', '')
            code = row.get('output', '')
            if len(code) > 20 and len(code) < 8000:
                yield format_llama3_instruct("You are a Python coding assistant.", prompt, code)
                count += 1
    except Exception as e: print(f"‚ùå Python Error: {e}")

def stream_wiki():
    print("üß† Init Wiki stream...")
    try:
        ds = load_dataset("wikimedia/wikipedia", "20231101.be", split="train", streaming=True)
        for row in ds:
            text = row.get('text', '')
            if is_pure_belarusian(text) and len(text) > MIN_TEXT_LENGTH:
                yield format_pretrain(text)
    except Exception as e: print(f"‚ùå Wiki Error: {e}")

def stream_cosmopedia():
    print("üá¨üáß Init Cosmopedia stream...")
    try:
        ds = load_dataset("HuggingFaceTB/cosmopedia", "stanford", split="train", streaming=True)
        count = 0
        for row in ds:
            if count >= LIMIT_LOGIC_EN: break
            yield format_pretrain(row['text'])
            count += 1
    except Exception as e: print(f"‚ùå Cosmopedia Error: {e}")

def stream_culturax():
    print("üåç Init CulturaX stream...")
    try:
        parquet_files = glob.glob(os.path.join(LOCAL_CULTURAX_PATH, "*.parquet"))
        if not parquet_files: return
        random.shuffle(parquet_files)
        
        c_web = 0
        for p_file in parquet_files:
            if c_web >= LIMIT_CULTURAX: break
            try:
                # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
                df = pd.read_parquet(p_file, columns=['text']).dropna()
                # –ò—Ç–µ—Ä–∏—Ä—É–µ–º—Å—è –ø–æ —Å—Ç—Ä–æ–∫–∞–º
                for text in df['text']:
                    if c_web >= LIMIT_CULTURAX: break
                    if len(text) < MIN_TEXT_LENGTH: continue
                    if is_pure_belarusian(text):
                        yield format_pretrain(text)
                        c_web += 1
                
                # –ß–∏—Å—Ç–∏–º –ø–∞–º—è—Ç—å –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞
                del df
                gc.collect()
                
            except Exception as e: continue
    except Exception as e: print(f"‚ùå CulturaX Error: {e}")

# ================= MAIN MIXER =================

if not os.path.exists(DATA_DIR): os.makedirs(DATA_DIR)

print(f"üöÄ Starting STREAMING Generation...")
print(f"üíæ Output: {OUTPUT_FILE}")

# 1. –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –∞–∫—Ç–∏–≤–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–≤
generators = [
    stream_books(),
    stream_alpaca(),
    stream_python(),
    stream_wiki(),
    stream_cosmopedia(),
    stream_culturax()
]

# –§–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ/—É–ø–∞–≤—à–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ã —Å—Ä–∞–∑—É (–ø—Ä–æ–±–Ω—ã–π —Å—Ç–∞—Ä—Ç)
active_gens = []
for g in generators:
    if g is not None:
        active_gens.append(g)

buffer = []
total_written = 0

with open(OUTPUT_FILE, "w", encoding="utf-8") as f_out:
    pbar = tqdm(desc="Processing & Mixing", unit=" docs")
    
    while active_gens:
        # 1. –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ (Random Selection)
        # –≠—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏–µ "–Ω–∞ –ª–µ—Ç—É"
        gen_idx = random.randint(0, len(active_gens) - 1)
        current_gen = active_gens[gen_idx]
        
        try:
            # 2. –ë–µ—Ä–µ–º 1 –¥–æ–∫—É–º–µ–Ω—Ç
            doc = next(current_gen)
            buffer.append(doc)
            
            # 3. –ï—Å–ª–∏ –±—É—Ñ–µ—Ä –ø–æ–ª–æ–Ω -> –°–±—Ä–æ—Å –Ω–∞ –¥–∏—Å–∫
            if len(buffer) >= BUFFER_SIZE:
                random.shuffle(buffer) # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –≤–Ω—É—Ç—Ä–∏ –±—É—Ñ–µ—Ä–∞
                for item in buffer:
                    f_out.write(item)
                total_written += len(buffer)
                pbar.update(len(buffer))
                buffer = [] # –û—á–∏—â–∞–µ–º RAM
                gc.collect()
                
        except StopIteration:
            # –ò—Å—Ç–æ—á–Ω–∏–∫ –∑–∞–∫–æ–Ω—á–∏–ª—Å—è, —É–¥–∞–ª—è–µ–º –∏–∑ —Å–ø–∏—Å–∫–∞
            active_gens.pop(gen_idx)
        except Exception as e:
            print(f"‚ö†Ô∏è Stream Error: {e}")
            active_gens.pop(gen_idx)

    # 4. –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –æ—Å—Ç–∞—Ç–∫–∏ –±—É—Ñ–µ—Ä–∞
    if buffer:
        random.shuffle(buffer)
        for item in buffer:
            f_out.write(item)
        total_written += len(buffer)
        pbar.update(len(buffer))

print(f"\nüéâ COMPLETE! Total Docs: {total_written}")
file_size_mb = os.path.getsize(OUTPUT_FILE) / (1024 * 1024)
print(f"üíæ Final Size: {file_size_mb:.2f} MB")