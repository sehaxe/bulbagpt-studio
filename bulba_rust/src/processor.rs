use pyo3::prelude::*;
use std::fs::File;
use std::io::{Read, Write, BufWriter};
use std::path::Path;
use tokenizers::{Tokenizer, PostProcessorWrapper};
use std::sync::{Arc, mpsc};
use std::thread;
use rayon::prelude::*;

// –†–∞–∑–º–µ—Ä –±–ª–æ–∫–∞ —á—Ç–µ–Ω–∏—è (64 MB).
// –ë–æ–ª—å—à–µ = –±—ã—Å—Ç—Ä–µ–µ (–º–µ–Ω—å—à–µ IO –≤—ã–∑–æ–≤–æ–≤), –Ω–æ –±–æ–ª—å—à–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ RAM.
const CHUNK_SIZE: usize = 64 * 1024 * 1024; 

#[pyfunction]
pub fn process_parallel(
    py: Python,
    files: Vec<String>,
    tokenizer_path: String,
    output_path: String
) -> PyResult<String> {
    
    py.allow_threads(move || {
        println!("RUST: üöÄ STARTING ENGINE (Chunked Read + Background Writer)...");

        // 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        let mut tokenizer_raw = Tokenizer::from_file(&tokenizer_path)
            .map_err(|e| PyErr::new::<pyo3::exceptions::PyRuntimeError, _>(format!("{}", e)))?;
        tokenizer_raw.with_post_processor(None::<PostProcessorWrapper>);
        let tokenizer = Arc::new(tokenizer_raw);

        // 2. –°–æ–∑–¥–∞–µ–º –∫–∞–Ω–∞–ª –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –¥–∞–Ω–Ω—ã—Ö Writer-–ø–æ—Ç–æ–∫—É
        // sync_channel —Å –±—É—Ñ–µ—Ä–æ–º, —á—Ç–æ–±—ã –Ω–µ –∑–∞–±–∏—Ç—å –≤—Å—é –ø–∞–º—è—Ç—å, –µ—Å–ª–∏ –¥–∏—Å–∫ –º–µ–¥–ª–µ–Ω–Ω—ã–π
        let (tx, rx) = mpsc::sync_channel::<Vec<u8>>(16);

        // 3. –ó–∞–ø—É—Å–∫–∞–µ–º Writer –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
        let output_path_clone = output_path.clone();
        let writer_handle = thread::spawn(move || -> std::io::Result<usize> {
            let file = File::create(output_path_clone)?;
            // –û–≥—Ä–æ–º–Ω—ã–π –±—É—Ñ–µ—Ä –∑–∞–ø–∏—Å–∏ (32 MB)
            let mut writer = BufWriter::with_capacity(32 * 1024 * 1024, file);
            let mut total_tokens = 0;

            for bytes in rx {
                writer.write_all(&bytes)?;
                // –ö–∞–∂–¥–æ–µ —á–∏—Å–ª–æ u16 –∑–∞–Ω–∏–º–∞–µ—Ç 2 –±–∞–π—Ç–∞
                total_tokens += bytes.len() / 2;
            }
            writer.flush()?;
            Ok(total_tokens)
        });

        // 4. –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –ø–æ —Ñ–∞–π–ª–∞–º
        let mut buffer = vec![0u8; CHUNK_SIZE];

        for path_str in files {
            let path = Path::new(&path_str);
            let mut file = match File::open(path) { Ok(f) => f, Err(_) => continue };
            
            let mut leftovers = Vec::new();

            loop {
                // –ß–∏—Ç–∞–µ–º –∫—É—Å–æ–∫ —Ñ–∞–π–ª–∞ –≤ –±—É—Ñ–µ—Ä
                let bytes_read = match file.read(&mut buffer) {
                    Ok(0) => break, // EOF
                    Ok(n) => n,
                    Err(_) => break,
                };

                let chunk = &buffer[..bytes_read];

                // –ù–∞–º –Ω—É–∂–Ω–æ –Ω–∞–π—Ç–∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏, —á—Ç–æ–±—ã –Ω–µ —Ä–∞–∑—Ä–µ–∑–∞—Ç—å —Å—Ç—Ä–æ–∫—É –ø–æ—Å–µ—Ä–µ–¥–∏–Ω–µ
                let (valid_chunk, rest) = if bytes_read == CHUNK_SIZE {
                    // –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π '\n'
                    match chunk.iter().rposition(|&b| b == b'\n') {
                        Some(pos) => (&chunk[..=pos], &chunk[pos+1..]),
                        None => (chunk, &[][..]), // –û—á–µ–Ω—å –¥–ª–∏–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ –∏–ª–∏ –∫–æ–Ω–µ—Ü, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
                    }
                } else {
                    (chunk, &[][..]) // –ö–æ–Ω–µ—Ü —Ñ–∞–π–ª–∞
                };

                // –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç: "–æ—Å—Ç–∞—Ç–∫–∏ —Å –ø—Ä–æ—à–ª–æ–≥–æ —Ä–∞–∑–∞" + "—Ç–µ–∫—É—â–∏–π –≤–∞–ª–∏–¥–Ω—ã–π –∫—É—Å–æ–∫"
                // unsafe –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏ (–º—ã –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ —Ñ–∞–π–ª—ã –≤–∞–ª–∏–¥–Ω—ã–π UTF-8).
                // –ï—Å–ª–∏ —Ñ–∞–π–ª—ã –º–æ–≥—É—Ç –±—ã—Ç—å –±–∏—Ç—ã–º–∏, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ String::from_utf8_lossy
                let text_to_process = if !leftovers.is_empty() {
                    leftovers.extend_from_slice(valid_chunk);
                    unsafe { String::from_utf8_unchecked(leftovers.clone()) }
                } else {
                    unsafe { String::from_utf8_unchecked(valid_chunk.to_vec()) }
                };

                // –°–æ—Ö—Ä–∞–Ω—è–µ–º "—Ö–≤–æ—Å—Ç" –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏
                leftovers = rest.to_vec();
                if bytes_read < CHUNK_SIZE && leftovers.is_empty() {
                    // –ï—Å–ª–∏ —ç—Ç–æ –±—ã–ª –∫–æ–Ω–µ—Ü —Ñ–∞–π–ª–∞ –∏ —Ö–≤–æ—Å—Ç–æ–≤ –Ω–µ—Ç, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Å—Ç–∞—Ç–æ–∫ –∏ –≤—ã—Ö–æ–¥–∏–º
                }

                if text_to_process.trim().is_empty() {
                    if bytes_read < CHUNK_SIZE { break; } // EOF
                    continue; 
                }

                // üî• PARALLEL PROCESSING üî•
                // Rayon —Å–∞–º —Ä–∞–∑–æ–±—å–µ—Ç text_to_process –Ω–∞ —Å—Ç—Ä–æ–∫–∏ (par_lines) –±–µ–∑ –ª–∏—à–Ω–∏—Ö –∞–ª–ª–æ–∫–∞—Ü–∏–π
                let processed_chunk: Vec<u8> = text_to_process
                    .par_lines()
                    .flat_map(|line| {
                        if line.is_empty() { return Vec::new(); }
                        
                        // –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
                        if let Ok(encoding) = tokenizer.encode(line, false) {
                            let ids = encoding.get_ids();
                            // –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è u32 -> u16 "–Ω–∞ –ª–µ—Ç—É" –∏ —Å—Ä–∞–∑—É –≤ –±–∞–π—Ç—ã
                            let mut byte_buf = Vec::with_capacity(ids.len() * 2);
                            for &id in ids {
                                if id != 0 {
                                    let id_u16 = id as u16;
                                    byte_buf.extend_from_slice(&id_u16.to_ne_bytes()); // Native Endian
                                }
                            }
                            byte_buf
                        } else {
                            Vec::new()
                        }
                    })
                    .collect(); // –°–æ–±–∏—Ä–∞–µ–º –æ–¥–∏–Ω –±–æ–ª—å—à–æ–π –±–∏–Ω–∞—Ä–Ω—ã–π –±–ª–æ–±

                // –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –±–ª–æ–± –ø–∏—Å–∞—Ç–µ–ª—é
                if !processed_chunk.is_empty() {
                    if tx.send(processed_chunk).is_err() {
                        break; // Writer —É–º–µ—Ä
                    }
                }

                if bytes_read < CHUNK_SIZE {
                    break; // –ö–æ–Ω–µ—Ü —Ñ–∞–π–ª–∞
                }
            }
        }

        // –ó–∞–∫—Ä—ã–≤–∞–µ–º –∫–∞–Ω–∞–ª, —á—Ç–æ–±—ã writer –ø–æ–Ω—è–ª, —á—Ç–æ –¥–∞–Ω–Ω—ã—Ö –±–æ–ª—å—à–µ –Ω–µ –±—É–¥–µ—Ç
        drop(tx);

        // –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∑–∞–ø–∏—Å–∏
        let final_count = writer_handle.join().unwrap().unwrap_or(0);
        
        println!("RUST: ‚úÖ DONE. Total tokens: {}", final_count);
        Ok(format!("{}", final_count))
    })
}