import torch
import os
import glob
from transformers import LlamaConfig, LlamaForCausalLM, LlamaTokenizerFast
from safetensors.torch import load_file

# ================= –ù–ê–°–¢–†–û–ô–ö–ò =================
TOKENIZER_PATH = "system/tokenizer"
CHECKPOINT_DIR = "checkpoints"
MODEL_TYPE = "AIst (150M)" 

PRESETS = {
    "Krolik (50M)":  { "h": 512,  "i": 1376, "l": 8,  "hd": 8,  "kv": 4, "ctx": 512 },
    "AIst (150M)":   { "h": 768,  "i": 2048, "l": 12, "hd": 12, "kv": 4, "ctx": 512 },
    "Zubr (350M)":   { "h": 1024, "i": 2816, "l": 28, "hd": 16, "kv": 4, "ctx": 512 },
}
# =============================================

def get_latest_checkpoint():
    files = glob.glob(f"{CHECKPOINT_DIR}/*.safetensors")
    if not files: return None
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è (—Å–∞–º—ã–π –Ω–æ–≤—ã–π - –ø–µ—Ä–≤—ã–π)
    return max(files, key=os.path.getmtime)

def clean_state_dict(sd):
    """–£–±–∏—Ä–∞–µ—Ç –ø—Ä–µ—Ñ–∏–∫—Å _orig_mod. (–æ—Ç torch.compile)"""
    new_sd = {}
    for k, v in sd.items():
        new_k = k.replace("_orig_mod.", "")
        new_sd[new_k] = v
    return new_sd

def generate_text():
    print(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏–∑ {TOKENIZER_PATH}...")
    try:
        tokenizer = LlamaTokenizerFast.from_pretrained(TOKENIZER_PATH)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞: {e}")
        return

    ckpt_path = get_latest_checkpoint()
    if not ckpt_path:
        print("‚ùå –ß–µ–∫–ø–æ–∏–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
        return
    print(f"üì• –í—ã–±—Ä–∞–Ω —á–µ–∫–ø–æ–∏–Ω—Ç: {ckpt_path}")

    print(f"üèóÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ {MODEL_TYPE}...")
    p = PRESETS[MODEL_TYPE]
    
    config = LlamaConfig(
        vocab_size=len(tokenizer),
        hidden_size=p["h"],
        intermediate_size=p["i"],
        num_hidden_layers=p["l"],
        num_attention_heads=p["hd"],
        num_key_value_heads=p["kv"],
        max_position_embeddings=p["ctx"],
        rope_theta=10000.0,
        attn_implementation="sdpa" # –£—Å–∫–æ—Ä—è–µ—Ç –∏–Ω—Ñ–µ—Ä–µ–Ω—Å
    )
    
    model = LlamaForCausalLM(config)
    
    print(f"üíæ –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤...")
    state_dict = load_file(ckpt_path)
    state_dict = clean_state_dict(state_dict) # –ß–∏—Å—Ç–∏–º –∫–ª—é—á–∏
    
    msg = model.load_state_dict(state_dict, strict=False)
    print(f"–°—Ç–∞—Ç—É—Å –∑–∞–≥—Ä—É–∑–∫–∏: {msg}")

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"üöÄ –ó–∞–ø—É—Å–∫ –Ω–∞ {device}")
    model.to(device)
    model.eval()

    while True:
        print("\n" + "="*40)
        prompt = input("üìù –£–≤—è–¥–∑—ñ—Ü–µ –∑–∞–ø—ã—Ç (—Ü—ñ 'q' –¥–ª—è –≤—ã—Ö–∞–¥—É): ")
        if prompt.lower() in ['q', 'exit']: break
        
        # –§–∞—Ä–º–∞—Ç Instruct (–∫–∞–ª—ñ —Ç—Ä—ç–±–∞)
        formatted_prompt = f"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
        
        inputs = tokenizer(formatted_prompt, return_tensors="pt").to(device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs, 
                max_new_tokens=100,
                temperature=0.6,
                top_k=40,
                repetition_penalty=1.15,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id
            )

        result = tokenizer.decode(outputs[0], skip_special_tokens=False)
        # –¢—Ä–æ—Ö—ñ —á—ã—Å—Ü—ñ–º –≤—ã–≤–∞–¥, –∫–∞–± –ø–∞–∫—ñ–Ω—É—Ü—å —Ç–æ–ª—å–∫—ñ –∞–¥–∫–∞–∑
        answer = result.split("assistant<|end_header_id|>\n\n")[-1].replace("<|eot_id|>", "").replace("<|end_of_text|>", "")
        
        print(f"ü§ñ –ê–¥–∫–∞–∑:\n{answer}")

if __name__ == "__main__":
    generate_text()